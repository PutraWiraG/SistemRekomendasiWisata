# -*- coding: utf-8 -*-
"""Submission-Sistem-Rekomendasi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1farhpX97aIP_Z_iIAD4UScaF7zc1u1sg

<h1>Nama : Putra Dwi Wira Gardha Yuniahans</h1><br>
<h1>Kampus : Universitas Pembangunan Nasional Veteran Jawatimur</h1>
"""

pip install wget

import wget
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt

url = 'https://drive.google.com/uc?export=download&id=1pm9x4P1MQ-5y_u2U_zecdifI8fY_tpyh'
output_dir = '/content'
file = wget.download(url, out=output_dir)

!unzip /content/DATASET.zip

"""<h1>Data Understanding</h1>"""

wisata = pd.read_csv('/content/DATASET/tourism_with_id.csv')
user = pd.read_csv('/content/DATASET/user.csv')
rating_wisata = pd.read_csv('/content/DATASET/tourism_rating.csv')

"""Total Data Keseluruhan Berdasarkan Place_Id<br>
nantinya project ini hanya merekomendasikan wista dari kote tertentu saja
"""

print('Jumlah Wisata :', len(wisata.Place_Id.unique()))
print('Jumlah Data User :', len(user.User_Id.unique()))
print('Jumlah Data Rating Wisata :', len(rating_wisata.Place_Id.unique()))
print('=============================')
print('Jumlah Semua Data Wisata :', len(wisata))
print('Jumlah Semua Data User :', len(user))
print('Jumlah Semua Data Rating Wisata :', len(rating_wisata))

"""<h1>Exploratory Data</h1>

Data Wisata
"""

wisata.info()

wisata.head()

print('Banyak Data :', len(wisata.Place_Id.unique()))
print('Nama Kota :', wisata.City.unique())

"""Pada table data diatas terdapat kolom yang tidak diperlukan, seperti unamed11, dan unamed12. maka langkah selanjutnya menghapus kolom tersebut dari database."""

wisata = wisata.drop('Unnamed: 11', axis=1)
wisata = wisata.drop('Unnamed: 12', axis=1)

wisata.head()

feature = 'City'
count = wisata[feature].value_counts()
percent = 100*wisata[feature].value_counts(normalize=True)
city = pd.DataFrame({'jumlah Sebaran Data Wisata /kota':count, 'persentase':percent.round(1)})
print(city)
count.plot(kind='bar', title=feature);

"""Data User"""

print(user.shape)

user.head()

"""Data Rating Wisata"""

rating_wisata.head()

rating_wisata.describe()

print('Jumlah User :', len(rating_wisata.User_Id.unique()))
print('Jumlah Wisata :', len(rating_wisata.Place_Id.unique()))
print('Jumlah Rating :', len(rating_wisata))

"""<h1>Data Preprocesing</h1>

Seperti tujuan dari proyek ini yaitu merekomendasi wisata yang berada di Surabaya, maka pada dataset wisata hanya menampilkan data wisata yang berada di surabaya
"""

wisata_surabaya = wisata[wisata['City']=='Surabaya']
wisata_surabaya

"""Selanjutnya pada data rating_wisata juga ditampilkan hanya rating wisata di Surabaya"""

rating_wisata_surabaya = pd.merge(rating_wisata, wisata_surabaya[['Place_Id']], how='right', on='Place_Id')
rating_wisata_surabaya

"""Selanjutnya pada dataset user juga akan ditampilkan hanya user yang memberi rating wisata di Surabaya"""

user_surabaya = pd.merge(user, rating_wisata_surabaya[['User_Id']], how='right', on='User_Id').drop_duplicates().sort_values('User_Id')
user_surabaya

"""Menggabungkan Data dengan Fitur nama Wisata"""

all_wisata_surabaya = rating_wisata_surabaya
all_wisata_surabaya

all_wisata_surabaya = pd.merge(all_wisata_surabaya, 
                               wisata_surabaya[['Place_Id','Place_Name','Category']], 
                               on='Place_Id', 
                               how='left')
all_wisata_surabaya

"""<h1>Data Preparation</h1>"""

all_wisata_surabaya.isnull().sum()

print('Banyak Data Wisata Surabaya :', len(all_wisata_surabaya.Place_Name.unique()))
print('Wisata Surabaya :', all_wisata_surabaya.Place_Name.unique())

preparation = all_wisata_surabaya
preparation.sort_values('Place_Id')

preparation = preparation.drop_duplicates('Place_Id')
preparation

feature = 'Category'
count = preparation[feature].value_counts()
percent = 100*preparation[feature].value_counts(normalize=True)
kategori = pd.DataFrame({'jumlah Sebaran Category Wisata Surabaya':count, 'persentase':percent.round(1)})
print(kategori)
count.plot(kind='bar', title=feature);

from os import pread
id_wisata = preparation['Place_Id'].tolist()
nama_wisata = preparation['Place_Name'].tolist()
kategori_wisata = preparation['Category'].tolist()

print(len(id_wisata))
print(len(nama_wisata))
print(preparation.Category.unique())

new_wisata = pd.DataFrame({
    'id': id_wisata,
    'nama_wisata': nama_wisata,
    'kategori_wisata': kategori_wisata
})
new_wisata

"""<h1>Pemodelan Menggunakan Content-Based Filtering</h1>"""

data = new_wisata
data.sample(5)

from sklearn.feature_extraction.text import CountVectorizer
 
# Inisialisasi TfidfVectorizer
tf = CountVectorizer() 

tf.fit(data['kategori_wisata']) 
tf.get_feature_names()

tfidf_matrix = tf.fit_transform(data['kategori_wisata'])
tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=data.nama_wisata
).sample(10, axis=1).sample(10, axis=0)

"""Menghitung Consine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['nama_wisata'], columns=data['nama_wisata'])
print('Shape:', cosine_sim_df.shape)
 
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def resto_recommendations(wisata_nama, similarity_data=cosine_sim_df, items=data[['nama_wisata', 'kategori_wisata']], k=5):
    index = similarity_data.loc[:,wisata_nama].to_numpy().argpartition(
        range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]    
    closest = closest.drop(wisata_nama, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

data[data.nama_wisata.eq('Taman Harmoni Keputih')]

resto_recommendations('Taman Harmoni Keputih')

"""<h1>Pemodelan Colaborative Filtering</h1>"""

import numpy as np 
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

df = rating_wisata_surabaya
df

user_ids = df['User_Id'].unique().tolist()
print('list userID: ', user_ids)
 
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)
 
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

wisata_id = df['Place_Id'].unique().tolist()
 
wisata_to_wisata_encoded = {x: i for i, x in enumerate(wisata_id)}
 
wisata_encoded_to_wisata = {i: x for i, x in enumerate(wisata_id)}

df['user'] = df['User_Id'].map(user_to_user_encoded)
 
df['wisata'] = df['Place_Id'].map(wisata_to_wisata_encoded)

num_users = len(user_to_user_encoded)
print(num_users)
 
num_wisata = len(wisata_to_wisata_encoded)
print(num_wisata)
 
df['Place_Ratings'] = df['Place_Ratings'].values.astype(np.float32)
 
min_rating = min(df['Place_Ratings'])
 
max_rating = max(df['Place_Ratings'])
 
print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_wisata, min_rating, max_rating
))

df = df.sample(frac=1, random_state=42)
df

x = df[['user', 'wisata']].values
 
y = df['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

class RecommenderNet(tf.keras.Model):
 
  def __init__(self, num_users, num_wisata, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_wisata = num_wisata
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.wisata_embedding = layers.Embedding( # layer embeddings resto
        num_wisata,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.wisata_bias = layers.Embedding(num_wisata, 1) # layer embedding resto bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    wisata_vector = self.wisata_embedding(inputs[:, 1]) # memanggil layer embedding 3
    wisata_bias = self.wisata_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_wisata = tf.tensordot(user_vector, wisata_vector, 2) 
 
    x = dot_user_wisata + user_bias + wisata_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_wisata, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

wisata_df = new_wisata
df = rating_wisata_surabaya
 
user_id = df.User_Id.sample(1).iloc[0]
wisata_visited_by_user = df[df.User_Id == user_id]
 
# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html 
wisata_not_visited = wisata_df[~wisata_df['id'].isin(wisata_visited_by_user.Place_Id.values)]['id'] 
wisata_not_visited = list(
    set(wisata_not_visited)
    .intersection(set(wisata_to_wisata_encoded.keys()))
)
 
wisata_not_visited = [[wisata_to_wisata_encoded.get(x)] for x in wisata_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_wisata_array = np.hstack(
    ([[user_encoder]] * len(wisata_not_visited), wisata_not_visited)
)

ratings = model.predict(user_wisata_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_wisata_ids = [
    wisata_encoded_to_wisata.get(wisata_not_visited[x][0]) for x in top_ratings_indices
]
 
print('Menampilkan Rekomendasi wisata untuk users: {}'.format(user_id))
print('===' * 9)
print('Wisata dengan rating tertinggi dari user')
print('----' * 8)
 
top_wisata_user = (
    wisata_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)
 
wisata_df_rows = wisata_df[wisata_df['id'].isin(top_wisata_user)]
for row in wisata_df_rows.itertuples():
    print(row.nama_wisata, ':', row.kategori_wisata)
 
print('----' * 8)
print('10 rekomendasi destinasi wisata surabaya')
print('----' * 8)
 
recommended_wisata = wisata_df[wisata_df['id'].isin(recommended_wisata_ids)]
for row in recommended_wisata.itertuples():
    print(row.nama_wisata, ':', row.kategori_wisata)